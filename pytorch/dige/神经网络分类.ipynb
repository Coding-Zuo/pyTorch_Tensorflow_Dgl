{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "L = [0, 10, 20, 30, 40, 50]\n",
    "low, high, stride = 1, 4, 2\n",
    "\n",
    "# 正序\n",
    "assert L[:] == [0, 10, 20, 30, 40, 50] # [seq[0],   seq[1],          ..., seq[-1]    ]\n",
    "assert L[low:] == [10, 20, 30, 40, 50] # [seq[low], seq[low+1],      ..., seq[-1]    ]\n",
    "assert L[:high] == [0, 10, 20, 30]     # [seq[0],   seq[1],          ..., seq[high-1]]\n",
    "assert L[low:high] == [10, 20, 30]     # [seq[low], seq[low+1],      ..., seq[high-1]]\n",
    "assert L[::stride] == [0, 20, 40]      # [seq[0],   seq[stride],     ..., seq[-1]    ]\n",
    "assert L[low::stride] == [10, 30, 50]  # [seq[low], seq[low+stride], ..., seq[-1]    ]\n",
    "assert L[:high:stride] == [0, 20]      # [seq[0],   seq[stride],     ..., seq[high-1]]\n",
    "assert L[low:high:stride] == [10, 30]  # [seq[low], seq[low+stride], ..., seq[high-1]]\n",
    "\n",
    "# 逆序\n",
    "assert L[::-stride] == [50, 30, 10]    # [seq[-1],   seq[-1-stride],   ..., seq[0]    ]\n",
    "assert L[high::-stride] == [40, 20, 0] # [seq[high], seq[high-stride], ..., seq[0]    ]\n",
    "assert L[:low:-stride] == [50, 30]     # [seq[-1],   seq[-1-stride],   ..., seq[low+1]]\n",
    "assert L[high:low:-stride] == [40, 20] # [seq[high], seq[high-stride], ..., seq[low+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从二级制数据读取文件\n",
    "import torch.utils.data as Data\n",
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "dataPath = '/home/zuoyuhui/datasets/fashion-mnist'\n",
    "\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "def load_data(data_folder, data_name, label_name):\n",
    "    \"\"\"\n",
    "        data_folder: 文件目录\n",
    "        data_name： 数据文件名\n",
    "        label_name：标签数据文件名\n",
    "    \"\"\"\n",
    "    with gzip.open(os.path.join(data_folder,label_name), 'rb') as lbpath: # rb表示的是读取二进制数据\n",
    "        y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n",
    " \n",
    "    with gzip.open(os.path.join(data_folder,data_name), 'rb') as imgpath:\n",
    "        x_train = np.frombuffer(\n",
    "            imgpath.read(), np.uint8, offset=16).reshape(len(y_train), 28, 28)\n",
    "    return (x_train, y_train)\n",
    "\n",
    "\n",
    "class DealDataset(Data.Dataset):\n",
    "    \"\"\"\n",
    "        读取数据、初始化数据\n",
    "    \"\"\"\n",
    "    def __init__(self, folder, data_name, label_name,transform=None):\n",
    "        (train_set, train_labels) = load_data(folder, data_name, label_name) # 其实也可以直接使用torch.load(),读取之后的结果为torch.Tensor形式\n",
    "        self.train_set = train_set\n",
    "        self.train_labels = train_labels\n",
    "        self.transform = transform\n",
    " \n",
    "    def __getitem__(self, index):\n",
    " \n",
    "        img, target = self.train_set[index], int(self.train_labels[index])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,y_train = load_data(dataPath,\n",
    "                           \"train-images-idx3-ubyte.gz\",\n",
    "                           \"train-labels-idx1-ubyte.gz\")\n",
    " \n",
    "import numpy as np\n",
    "x_train = np.array(x_train)\n",
    "x_valid = x_train[-10000:]\n",
    "x_train = x_train[:50000]\n",
    "y_train = np.array(y_train)\n",
    "y_valid = y_train[-10000:]\n",
    "y_train = y_train[:50000]\n",
    "\n",
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f075642bf10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_valid[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 28, 28])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,y_train,x_valid,y_valid = map(\n",
    "    torch.tensor,(x_train,y_train,x_valid,y_valid)\n",
    ")\n",
    "\n",
    "n,c,_=x_train.cpu().data.numpy().shape\n",
    "c = c**2\n",
    "\n",
    "print(y_train.min())\n",
    "x_train.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如果模型有可学习的参数最好用nn.Module  其他情况用torch.nn.functional相对简单 \n",
    "\n",
    "- 创建一样高类必须继承nn.Module且在其构造函数中需调用nn.Module的构造函数\n",
    "- 无需写方向传播，nn.Module能利用autograd自动实现反向传播\n",
    "- Module中的可学习参数可以通过named_parameters()或者parameters()返回迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb.mm(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., -0., -0., 0.],\n",
       "         [0., 0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., 0., 0.,  ..., -0., 0., 0.],\n",
       "         ...,\n",
       "         [-0., -0., -0.,  ..., 0., -0., 0.],\n",
       "         [-0., -0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., -0., -0.,  ..., -0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -0., -0., 0.],\n",
       "         [0., 0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., 0., 0.,  ..., -0., 0., 0.],\n",
       "         ...,\n",
       "         [-0., -0., -0.,  ..., 0., -0., 0.],\n",
       "         [-0., -0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., -0., -0.,  ..., -0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -0., -0., 0.],\n",
       "         [0., 0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., 0., 0.,  ..., -0., 0., 0.],\n",
       "         ...,\n",
       "         [-0., -0., -0.,  ..., 0., -0., 0.],\n",
       "         [-0., -0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., -0., -0.,  ..., -0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., -0., -0., 0.],\n",
       "         [0., 0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., 0., 0.,  ..., -0., 0., 0.],\n",
       "         ...,\n",
       "         [-0., -0., -0.,  ..., 0., -0., 0.],\n",
       "         [-0., -0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., -0., -0.,  ..., -0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -0., -0., 0.],\n",
       "         [0., 0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., 0., 0.,  ..., -0., 0., 0.],\n",
       "         ...,\n",
       "         [-0., -0., -0.,  ..., 0., -0., 0.],\n",
       "         [-0., -0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., -0., -0.,  ..., -0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -0., -0., 0.],\n",
       "         [0., 0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., 0., 0.,  ..., -0., 0., 0.],\n",
       "         ...,\n",
       "         [-0., -0., -0.,  ..., 0., -0., 0.],\n",
       "         [-0., -0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., -0., -0.,  ..., -0., 0., 0.]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "xb = x_train[0:batch_size]\n",
    "yb = y_train[0:batch_size]\n",
    "\n",
    "weights = torch.randn([28,28,10],dtype=torch.float, requires_grad = True)\n",
    "bias = torch.zeros(10,requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "res = torch.einsum('bww,wwj->bwj', xb, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 28, 28]), torch.Size([28, 28, 10]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape,weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 64, 28])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.permute(2, 0, 1).float().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
