{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从二级制数据读取文件\n",
    "import torch.utils.data as Data\n",
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "dataPath = '/home/zuoyuhui/datasets/fashion-mnist'\n",
    "\n",
    "def load_data(data_folder, data_name, label_name):\n",
    "    \"\"\"\n",
    "        data_folder: 文件目录\n",
    "        data_name： 数据文件名\n",
    "        label_name：标签数据文件名\n",
    "    \"\"\"\n",
    "    with gzip.open(os.path.join(data_folder,label_name), 'rb') as lbpath: # rb表示的是读取二进制数据\n",
    "        y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n",
    " \n",
    "    with gzip.open(os.path.join(data_folder,data_name), 'rb') as imgpath:\n",
    "        x_train = np.frombuffer(\n",
    "            imgpath.read(), np.uint8, offset=16).reshape(len(y_train), 28, 28)\n",
    "    return (x_train, y_train)\n",
    "\n",
    "\n",
    "class DealDataset(Data.Dataset):\n",
    "    \"\"\"\n",
    "        读取数据、初始化数据\n",
    "    \"\"\"\n",
    "    def __init__(self, folder, data_name, label_name,transform=None):\n",
    "        (train_set, train_labels) = load_data(folder, data_name, label_name) # 其实也可以直接使用torch.load(),读取之后的结果为torch.Tensor形式\n",
    "        self.train_set = train_set\n",
    "        self.train_labels = train_labels\n",
    "        self.transform = transform\n",
    " \n",
    "    def __getitem__(self, index):\n",
    " \n",
    "        img, target = self.train_set[index], int(self.train_labels[index])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 实例化这个类，然后我们就得到了Dataset类型的数据，记下来就将这个类传给DataLoader，就可以了。\n",
    "trainDataset = DealDataset(dataPath,\n",
    "                           \"train-images-idx3-ubyte.gz\",\n",
    "                           \"train-labels-idx1-ubyte.gz\",\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.ToTensor()\n",
    "                               transforms.Normalizemalize((0.2860,),(0.3530,))\n",
    "                           ]))\n",
    " \n",
    "testDataset = DealDataset(dataPath, train=False,\n",
    "                          \"t10k-images-idx3-ubyte.gz\",\n",
    "                          \"t10k-labels-idx1-ubyte.gz\",\n",
    "                          transform=transforms.Compose([\n",
    "                               transforms.ToTensor()\n",
    "                               transforms.Normalizemalize((0.2860,),(0.3530,))\n",
    "                           ]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional.py:74: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729006826/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset[223][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2860402 0.3530239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [d[0].data.cpu().numpy() for d in trainDataset]\n",
    "print(np.mean(data),np.std(data)) # 所有图片像素的均值方差\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainDataset.train_labels.shape:(60000,)\n",
      "\n",
      "trainDataset.train_set.shape:(60000, 28, 28)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10个类别的图像，分别是：t-shirt（T恤），trouser（牛仔裤），pullover（套衫），dress（裙子），coat（外套），sandal（凉鞋），shirt（衬衫），sneaker（运动鞋），bag（包），ankle boot（短靴）。\n",
    "# 这里trainDataset包含:train_labels, train_set等属性;  数据类型均为ndarray\n",
    "print(f'trainDataset.train_labels.shape:{trainDataset.train_labels.shape}\\n')\n",
    "print(f'trainDataset.train_set.shape:{trainDataset.train_set.shape}\\n')\n",
    " \n",
    "# 这里train_loader包含:batch_size、dataset等属性，数据类型分别为int，DealDataset\n",
    "# dataset中又包含train_labels, train_set等属性;  数据类型均为ndarray\n",
    "# print(f'train_loader.batch_size: {train_loader.batch_size}\\n')\n",
    "# print(f'train_loader.dataset.train_labels.shape: {train_loader.dataset.train_labels.shape}\\n')\n",
    "# print(f'train_loader.dataset.train_set.shape: {train_loader.dataset.train_set.shape}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1) # 28 * 28 -> (28+1-5) 24 * 24\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1) # 20 * 20\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: 1 * 28 * 28\n",
    "        x = F.relu(self.conv1(x)) # 20 * 24 * 24\n",
    "        x = F.max_pool2d(x,2,2) # 12 * 12\n",
    "        x = F.relu(self.conv2(x)) # 8 * 8\n",
    "        x = F.max_pool2d(x,2,2) # 4 *4 \n",
    "        x = x.view(-1, 4*4*50) # reshape (5 * 2 * 10), view(5, 20) -> (5 * 20)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x= self.fc2(x)\n",
    "        # return x\n",
    "        return F.log_softmax(x, dim=1) # log probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for idx,(data,target) in enumerate(train_loader):\n",
    "        data,target = data.to(device), target.to(device)\n",
    "        \n",
    "        pred = model(data)\n",
    "        # NLLLoss 函数输入 input 之前，需要对 input 进行 log_softmax 处理\n",
    "        loss = F.nll_loss(pred,target) # 对每个标签对应列进行绝对值取平均\n",
    "        \n",
    "        #SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % 100 ==0:\n",
    "            print('Train Epoch:{}, iteration:{} , Loss:{}'.format(epoch,idx,loss.item()))\n",
    "            \n",
    "            \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss=0.\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for idx,(data,target) in enumerate(test_loader):\n",
    "            data,target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data) # batch_size*10 找出最大的\n",
    "            # NLLLoss 函数输入 input 之前，需要对 input 进行 log_softmax 处理\n",
    "            total_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # 对每个标签对应列进行绝对值取求和\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "                \n",
    "    total_loss /= len(test_loader.dataset)\n",
    "    acc = correct/len(test_loader.dataset) * 100.\n",
    "    print('Test loss:{},Accuracy:{}'.format(total_loss,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:0, iteration:0 , Loss:2.294975996017456\n",
      "Train Epoch:0, iteration:100 , Loss:1.3832156658172607\n",
      "Train Epoch:0, iteration:200 , Loss:0.8337352275848389\n",
      "Train Epoch:0, iteration:300 , Loss:0.917647659778595\n",
      "Train Epoch:0, iteration:400 , Loss:0.5856406092643738\n",
      "Train Epoch:0, iteration:500 , Loss:0.6871902942657471\n",
      "Train Epoch:0, iteration:600 , Loss:0.8883571624755859\n",
      "Train Epoch:0, iteration:700 , Loss:0.6252657771110535\n",
      "Train Epoch:0, iteration:800 , Loss:0.8801292181015015\n",
      "Train Epoch:0, iteration:900 , Loss:0.7535462975502014\n",
      "Train Epoch:0, iteration:1000 , Loss:0.515783965587616\n",
      "Train Epoch:0, iteration:1100 , Loss:0.6451463103294373\n",
      "Train Epoch:0, iteration:1200 , Loss:0.6824079751968384\n",
      "Train Epoch:0, iteration:1300 , Loss:0.4769243597984314\n",
      "Train Epoch:0, iteration:1400 , Loss:0.5080010294914246\n",
      "Train Epoch:0, iteration:1500 , Loss:0.7674784660339355\n",
      "Train Epoch:0, iteration:1600 , Loss:0.9188860654830933\n",
      "Train Epoch:0, iteration:1700 , Loss:0.4896189272403717\n",
      "Train Epoch:0, iteration:1800 , Loss:0.55879145860672\n",
      "Test loss:0.5799746597290039,Accuracy:77.32\n",
      "Train Epoch:1, iteration:0 , Loss:0.48191410303115845\n",
      "Train Epoch:1, iteration:100 , Loss:0.3420575261116028\n",
      "Train Epoch:1, iteration:200 , Loss:0.7553240060806274\n",
      "Train Epoch:1, iteration:300 , Loss:0.4624951481819153\n",
      "Train Epoch:1, iteration:400 , Loss:0.8179327845573425\n",
      "Train Epoch:1, iteration:500 , Loss:0.6041240096092224\n",
      "Train Epoch:1, iteration:600 , Loss:0.3513946533203125\n",
      "Train Epoch:1, iteration:700 , Loss:0.4019157886505127\n",
      "Train Epoch:1, iteration:800 , Loss:0.4206518232822418\n",
      "Train Epoch:1, iteration:900 , Loss:0.4070601463317871\n",
      "Train Epoch:1, iteration:1000 , Loss:0.581283688545227\n",
      "Train Epoch:1, iteration:1100 , Loss:0.6365148425102234\n",
      "Train Epoch:1, iteration:1200 , Loss:0.5826618075370789\n",
      "Train Epoch:1, iteration:1300 , Loss:0.6221364140510559\n",
      "Train Epoch:1, iteration:1400 , Loss:0.40928414463996887\n",
      "Train Epoch:1, iteration:1500 , Loss:0.3947979509830475\n",
      "Train Epoch:1, iteration:1600 , Loss:0.5376114845275879\n",
      "Train Epoch:1, iteration:1700 , Loss:0.8581283092498779\n",
      "Train Epoch:1, iteration:1800 , Loss:0.5909932255744934\n",
      "Test loss:0.45966129512786863,Accuracy:83.53\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 32\n",
    "# 训练数据和测试数据的装载\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=trainDataset,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True, #可让训练速度变快\n",
    ")\n",
    " \n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=testDataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True, #可让训练速度变快\n",
    ")\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lr,momentum=momentum)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    \n",
    "    \n",
    "torch.save(model.state_dict() , 'fm_cnn.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
