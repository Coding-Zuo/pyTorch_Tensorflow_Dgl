{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量\n",
    "- 学习词向量的概念\n",
    "- 用skip-thought模型训练词向量\n",
    "- 学习使用pytorch dataset和dataloader\n",
    "- 学习定义pytorch模型\n",
    "- 学习torch.nn 中常见的Module\n",
    "    - embedding\n",
    "- 学习常见的pytorch operations\n",
    "   - bmm\n",
    "   - logsigmoid\n",
    "- 保存和读取模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Representations of words and Phrases and their Compositionality\n",
    "Skip-gram 模型，使用论文中的noice contrastive sampling的目标函数，没有实现subsampling：论文section2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math \n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)\n",
    "    \n",
    "    \n",
    "# 定义一些超参数\n",
    "C = 3 # context window 周围三个单词\n",
    "K = 100 # number of negatuve samples 每出现一个正确的词要出现100个错误的词。\n",
    "\n",
    "\n",
    "BATCH_SIZE=128\n",
    "NUM_EPOCHS=2\n",
    "learning_rate = 0.2\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wird_tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从文本文件中读取所有文字，通过这些文本创建一个vocabulary\n",
    "- 由于单词数量可能太大，我们只选取常见的MAX_VOCAB_SIZE个单词\n",
    "- 添加一个UNK单词表示所有不常见的单词\n",
    "- 需要记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalizaed) frequncy,以及单词总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anarchism originated as a term of abuse first used against early working class radicals including th'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/text8.train.txt','r') as fin:\n",
    "    text = fin.read()\n",
    "    \n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.split()\n",
    "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1))\n",
    "vocab['<unk>'] = len(text) - np.sum(list(vocab.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = [word for word in vocab.keys()]\n",
    "word_to_idx = {word:i for i,word in enumerate(idx_to_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.array([count for count in vocab.values()], dtype=np.float32)\n",
    "word_freqs = word_counts/np.sum(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把概率提高到四分之三次方再 重新normalize\n",
    "# 3/4次方之后，会将高概率的单词的概率值，分一部分给低概率的单词，因为相同的操作，对高概率单词的概率值影响更大\n",
    "word_freqs = word_freqs**(3./4.)\n",
    "# 归一化\n",
    "word_freqs = word_freqs/np.sum(word_freqs)\n",
    "\n",
    "VOCAB_SIZE = len(idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.6247008e-02, 1.0514009e-02, 8.0499463e-03, ..., 5.0115582e-06,\n",
       "       5.0115582e-06, 1.1669193e-02], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现Dataloader\n",
    "一个dataloader 需要以下内容：\n",
    "- 把所有text编码成数字，然后用subsampling预处理这些文字\n",
    "- 保存vocabulary，单词count，normalized word frequency\n",
    "- 每个iteration sample 一个中心词\n",
    "- 根据当前的中心词返回context单词\n",
    "- 根据中心词sample一些negative单词\n",
    "- 返回单词的counts\n",
    "\n",
    "为了使用dataloader 需要定义两个function:\n",
    "- __len__ 需要返回整个数据集中有多少个item\n",
    "- __get__ 根据给定的index返回一个item\n",
    "\n",
    "有了dataloader后，可以轻松随机打乱整个数据集，拿到一个batch数据等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个dataset\n",
    "class WordEmbeddingDataset(tud.Dataset):\n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):\n",
    "        super(WordEmbeddingDataset,self).__init__()\n",
    "        self.text_encoded = [word_to_idx.get(word,word_to_idx['<unk>']) for word in text]\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded)\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 这个数据集一共有多少个item\n",
    "        return len(self.text_encoded)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "#         center_word = self.text_encoded[idx]\n",
    "#         pos_indexes = list(range(idx-C, idx)) + list(range(idx+1, idx+C+1)) \n",
    "#         pos_indices = [i%len(self.text_encoded) for i in pos_indices] \n",
    "#         pos_words = self.text_encoded[pos_indexes] \n",
    "#         neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0],True) # 傅立采样单词\n",
    "#         \n",
    "    \n",
    "        center_word = self.text_encoded[idx]\n",
    "        pos_indices = list(range(idx-C, idx)) + list(range(idx+1, idx+C+1))# window内单词的index\n",
    "        pos_indices = [i%len(self.text_encoded) for i in pos_indices]# 取余，防止超出text长度\n",
    "        pos_words = self.text_encoded[pos_indices] # 周围单词\n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)# 傅立采样单词\n",
    "        return center_word,pos_words,neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)\n",
    "dataloader = tud.DataLoader(dataset,batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-12-87a8fc0fea77>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-87a8fc0fea77>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    def forward(self,input_labels,pos_labels,neg_labels):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    # 差embedding multinomial bmm\n",
    "    def __init__(self,vocab_size,embed_size):\n",
    "        ''' 初始化输出和输出embedding\n",
    "        '''\n",
    "        super(EmbeddingModel,self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        initrange = 0.5 / self.embed_size\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self,input_labels,pos_labels,neg_labels):\n",
    "        # input_label:[batch_size]\n",
    "        # pos_labels:[batch_size,(window_size * 2)]\n",
    "        # neg_labels:[batch_size,(window_size*2*K)]\n",
    "        '''\n",
    "        input_labels: 中心词, [batch_size]\n",
    "        pos_labels: 中心词周围 context window 出现过的单词 [batch_size * (window_size * 2)]\n",
    "        neg_labelss: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (window_size * 2 * K)]\n",
    "        \n",
    "        return: loss, [batch_size]\n",
    "        '''\n",
    "        batch_size = input_labels.size(0)\n",
    "        input_embedding = self.in_embed(input_labels) # [batch_size,embed_size]\n",
    "        pos_embedding = self.out_embed(pos_embedding) # [batch_size,(window_size*2)]\n",
    "        neg_embedding = self.out_embed(neg_embedding) # [batch_size,(window_size*2*k),embed_size]\n",
    "        \n",
    "        input_embedding = input_embedding.unsquuze(2) # [batch_size,embed_size,1]\n",
    "        \n",
    "        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(2)).squeeze() # B * (2*C)\n",
    "        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(2)).squeeze() # B * (2*C*K)\n",
    "\n",
    "        log_pos = F.logsigmoid(log_pos).sum(1)\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1) # batch_size\n",
    "       \n",
    "        loss = log_pos + log_neg\n",
    "        \n",
    "        return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
